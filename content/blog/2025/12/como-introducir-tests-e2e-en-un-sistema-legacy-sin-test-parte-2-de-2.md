---
title: 'C√≥mo introducir tests E2E en un sistema legacy sin tests (parte 2/2)'
description: 'C√≥mo convertir el comportamiento real de un sistema legacy en un contrato verificable usando tests E2E y Approval Tests.'
author: pedropardal
date: 2025-12-28T00:00:00.000Z
layout: post
tags: [legacy, testing, arquitectura]
images: [/images/blog/posts/programador.jpg]
featured_image: /images/blog/posts/programador.jpg
card_image: /images/blog/posts/programador.jpg
---

En el [post anterior](./como-introducir-tests-e2e-en-un-sistema-legacy-sin-test-parte-1-de-2.md) llegamos a un punto clave: conseguimos ejecutar **un flujo cr√≠tico completo** dentro de un test.

Eso, por s√≠ solo, ya cambia las reglas del juego.

Pero ejecutar el flujo no es suficiente. El verdadero reto empieza ahora: ¬øC√≥mo validas ese comportamiento sin escribir tests fr√°giles, llenos de asserts irrelevantes, que se rompen con cualquier cambio menor?

Este post va exactamente de eso.

---

## Qu√© quer√≠amos validar (y qu√© no)

Antes de hablar de frameworks, asserts o snapshots, hubo que responder a una pregunta inc√≥moda, pero fundamental:

**¬øQu√© significa realmente que este flujo ‚Äúfuncione correctamente‚Äù?**

La respuesta **no** era:

* que se llame a tal m√©todo,
* que se ejecute tal query concreta,
* que se dispare tal evento interno en un punto exacto.

Ese tipo de validaciones acaban generando tests extremadamente fr√°giles, acoplados a la implementaci√≥n, que se rompen con refactors perfectamente leg√≠timos.

Aqu√≠ aparece adem√°s una trampa muy habitual en sistemas legacy: **la falsa sensaci√≥n de seguridad que dan ciertos tests unitarios**. Tests que pasan en verde, pero que solo validan fragmentos aislados del sistema, sin garantizar que el flujo completo siga funcionando cuando todas las piezas interact√∫an entre s√≠.

El problema no es el unit testing en s√≠, sino usarlo como sustituto de algo que no puede cubrir: la interacci√≥n real entre decisiones, estado persistido e integraciones externas.

Lo que quer√≠amos validar era algo m√°s grueso, pero tambi√©n mucho m√°s valioso:

* qu√© decisiones tom√≥ el sistema,
* qu√© efectos persistentes produjo,
* y qu√© historia completa cont√≥ durante la ejecuci√≥n del flujo.

En otras palabras: quer√≠amos validar **el comportamiento observable del sistema**, no su implementaci√≥n interna. Reducir el riesgo real de cambiar c√≥digo cr√≠tico, no simplemente aumentar el n√∫mero de tests en verde.

---

Buen punto, faltaba **cerrar el tri√°ngulo completo**: inputs ‚Üí sistema ‚Üí outputs.
Te dejo **la secci√≥n reescrita**, integrando los **inputs controlados** de forma clara y natural, sin alargarla innecesariamente.

---

## El test E2E como ‚Äúcaja negra controlada‚Äù

A partir de ah√≠, el test E2E se plante√≥ deliberadamente como una caja negra. La idea no era inspeccionar el interior del sistema, sino observar su comportamiento desde fuera, controlando con precisi√≥n qu√© entra y qu√© sale:

* le damos **una entrada conocida**,
* dejamos que el sistema haga lo suyo,
* y observamos qu√© ha pasado.

Nada de asserts finos repartidos por todo el c√≥digo.
Nada de mocks internos ni expectativas sobre llamadas concretas.

### Inputs controlados

Para que este enfoque funcione, los **inputs del sistema tienen que estar completamente bajo control**. En este caso, esos inputs eran b√°sicamente dos:

1. **El payload del webhook**, que representa el evento externo que inicia el flujo.
   Ese payload se construye expl√≠citamente en el test, con datos realistas pero deterministas: el pedido, el cliente, los importes, los estados relevantes. No llega por HTTP real, pero es exactamente el mismo contenido que llegar√≠a en producci√≥n.

2. **Las respuestas de las dependencias externas**, que ya no vienen del mundo real, sino de *dobles de test* (stubs) controlados.
   APIs externas, proveedores de pago, plataformas de terceros‚Ä¶ todo eso responde con datos predefinidos que el test decide de antemano.

De esta forma, el test define **el mundo exterior completo** en el que el sistema va a operar.

```php
public function test_create_order_from_shopify_webhook(): void {
    // Arrange
    $shopifyOrderId = //...
    $shopifyOrderData = ShopifyMother::createOrder($shopifyOrderId);
    $factory->fake()->withOrder($shopifyOrderId, $shopifyOrderData);

    // Act
    $payload = [
        // ...
    ];
    $this->shopifyWebhookListener->onOrderCreated($payload);

    // Assert
    // ...
}
```

### Fuentes de verdad observables

Con los inputs totalmente controlados, el test se apoya en **fuentes de verdad observables** para entender c√≥mo se ha comportado el sistema:

1. **El estado final de la base de datos**, que refleja los efectos persistentes del flujo.
2. **La traza de ejecuci√≥n**, que cuenta qu√© decisiones se tomaron y en qu√© orden.
3. **Los eventos que se dispararon**, que muestran c√≥mo reaccion√≥ el sistema y qu√© procesos secundarios se pusieron en marcha.

Estas tres vistas, combinadas con inputs completamente deterministas, permiten responder a la pregunta importante: no solo *‚Äú¬øtermin√≥ bien?‚Äù*, sino *‚Äú¬øse comport√≥ el sistema exactamente igual dado este escenario?‚Äù*.

A partir de aqu√≠, el reto deja de ser ejecutar el flujo y pasa a ser **c√≥mo capturar y comparar todo esto sin que los tests se vuelvan fr√°giles**, que es donde entra el resto del enfoque.

---

## La base de datos como parte del contrato

Empecemos por la base de datos.

En este tipo de sistemas, una parte muy importante del comportamiento real queda reflejada ah√≠: estados de pedidos, transiciones, flags, registros auxiliares, tablas intermedias‚Ä¶ Si el flujo ha cambiado de verdad, **la base de datos lo delata**.

La idea no fue comprobar *una* fila concreta ni validar campos individuales. El objetivo era capturar **una vista coherente del estado relevante del sistema** tras ejecutar el flujo completo.

### En la pr√°ctica

Al final de la ejecuci√≥n del test, se extrae un snapshot de la base de datos. Pero aqu√≠ se tom√≥ una decisi√≥n consciente: **no intentar ser fino demasiado pronto**.

En lugar de seleccionar columnas concretas y razonar una a una cu√°les deber√≠an cambiar y cu√°les no, el test captura **todo el contenido de las tablas que forman parte del flujo**. Tal cual.

Algo as√≠:

```php
$dbSnapshot = [
    'orders' => $this->fetchAll('SELECT * FROM orders'),
    'order_logs' => $this->fetchAll('SELECT * FROM order_logs'),
    'payments' => $this->fetchAll('SELECT * FROM payments'),
];
```

¬øEs ruidoso? S√≠.

¬øIncluye campos que quiz√° no importan a largo plazo? Tambi√©n.

Pero en este punto del proyecto, esa es precisamente la intenci√≥n.

Aqu√≠ el objetivo no es expresar reglas de negocio de forma precisa, sino **detectar cualquier cambio inesperado** en el comportamiento persistente del sistema. Si algo cambia en estas tablas como consecuencia de un refactor, queremos verlo. **Todo**.

M√°s adelante, cuando exista una red m√≠nima de seguridad y se empiecen a introducir tests m√°s espec√≠ficos ‚Äîunitarios o de integraci√≥n m√°s fina‚Äî tendr√° sentido ir acotando: seleccionar columnas concretas, aislar invariantes, expresar expectativas m√°s precisas.

Como primer test E2E de caracterizaci√≥n, **capturar todo el estado de las tablas implicadas en el flujo es una ventaja, no un defecto**.

---

## Los logs: la historia completa del sistema

Aqu√≠ es donde la observabilidad previa se vuelve clave.

Los logs no se usaron solo para debug, sino como **una segunda fuente de verdad** del comportamiento del sistema. Porque los logs bien estructurados cuentan cosas que la base de datos no siempre refleja:

* qu√© ramas se tomaron,
* qu√© reglas se evaluaron,
* qu√© integraciones se llamaron (o no),
* en qu√© orden ocurrieron las cosas.

### Captura de logs en tests

A nivel t√©cnico, el enfoque fue simple:

* configurar Monolog con un handler en memoria,
* limitarlo a los canales relevantes del flujo,
* capturar los logs estructurados durante la ejecuci√≥n del test.

Esquem√°ticamente:

```php
$logs = $this->logRecorder->records();
// cada record ya es un array estructurado
```

El resultado es una secuencia de eventos que describe **c√≥mo pens√≥ el sistema** mientras ejecutaba el flujo:

```json
{
    "channel": "webhook",
    "level": "INFO",
    "message": "Received orders/create webhook",
    "context": {
        "financial_status": "authorized",
        "shopify_order_id": 1234
    }
},
{
    "channel": "orders",
    "level": "INFO",
    "message": "Syncing order with Shopify...",
    "context": {
        "shopify_order_id": 1234
    },
},
{
    "channel": "orders",
    "level": "INFO",
    "message": "Synced order with Shopify with status = {order_status}, Shopify status = {shopify_status}",
    "context": {
        "shopify_order_id": 1234,
        "order_status": "new",
        "shopify_status": "authorized"
    },
},
//...
{
    "channel": "orders",
    "level": "INFO",
    "message": "New order added",
    "context": {
        "shopify_order_id": 1234
    },
},
```

---

## Capturar los eventos despachados

Adem√°s del estado final y de los logs, hab√≠a una tercera se√±al clave del comportamiento del sistema: **los mensajes despachados a trav√©s de Symfony Messenger**.

Muchos efectos importantes no ocurren de forma s√≠ncrona. Un flujo puede completarse ‚Äúbien‚Äù y, aun as√≠, cambiar radicalmente su comportamiento si deja de despachar ciertos mensajes o empieza a despachar otros distintos. Por eso, el test necesitaba observar **qu√© trabajo decidi√≥ poner en marcha el sistema**.

En el entorno de test, el bus de mensajes se instrumenta para **registrar todos los mensajes despachados** durante la ejecuci√≥n del flujo. No para inspeccionar handlers ni validar efectos secundarios todav√≠a, sino para capturar la intenci√≥n del sistema: qu√© procesos as√≠ncronos decidi√≥ disparar como consecuencia de la entrada recibida.

Esa lista de mensajes se convierte en una se√±al m√°s del comportamiento observable del sistema, complementando el estado persistido y la traza de logs.

---

## Unirlo todo: estado, historia y consecuencias

El punto clave fue entender que **ninguna de estas se√±ales por separado era suficiente**.

* **Solo base de datos** ‚Üí sabes *qu√©* cambi√≥, pero no *por qu√©*.
* **Solo logs** ‚Üí sabes *por qu√©*, pero no si el estado final es coherente.
* **Solo eventos** ‚Üí sabes *qu√© se intent√≥ hacer despu√©s*, pero no si el flujo lleg√≥ al estado esperado.

Fue la combinaci√≥n de las tres lo que dio una imagen fiel del comportamiento real del sistema.

Por eso, el output efectivo del test termin√≥ siendo una estructura compuesta, algo as√≠:

```php
$output = [
    'database' => $dbSnapshot,
    'logs' => $logSnapshot,
    'events' => $dispatchedMessages,
];
```

Ese objeto no representa una implementaci√≥n ni una serie de asserts aislados. Representa **la historia completa del flujo**:

* qu√© efectos persistentes produjo,
* qu√© decisiones se tomaron durante la ejecuci√≥n,
* y qu√© consecuencias as√≠ncronas decidi√≥ disparar el sistema.

Eso es lo que quer√≠amos proteger frente a refactors y cambios agresivos.

---

## Approval Tests: el encaje natural

Llegados a este punto, el tipo de test casi se elige solo.

Tenemos un output grande, estructurado y con mucho significado: estado de base de datos, traza de logs y eventos despachados. Comparar eso ‚Äúa mano‚Äù, campo a campo, con asserts tradicionales no solo ser√≠a tedioso, sino contraproducente. El test acabar√≠a lleno de detalles irrelevantes y se volver√≠a fr√°gil con cualquier cambio razonable.

Lo que realmente necesit√°bamos era otra cosa:

* poder **ver el resultado completo** del flujo,
* revisarlo con calma y criterio,
* y que cualquier cambio futuro se mostrara de forma clara y comprensible.

Ah√≠ es donde encajan los **Approval Tests**.

La idea detr√°s de este tipo de tests es simple, pero poderosa: en lugar de describir el resultado esperado con asserts, **guardas una representaci√≥n completa del resultado y la ‚Äúapruebas‚Äù conscientemente**. A partir de ese momento, ese snapshot se convierte en el contrato.

### C√≥mo funciona en la pr√°ctica

En este contexto concreto, el test sigue siempre el mismo esquema:

1. Ejecuta el flujo completo con inputs controlados.
2. Construye el output agregado (base de datos, logs y eventos).
3. Lo serializa a un formato legible, normalmente JSON.
4. Lo compara contra un snapshot previamente aprobado.

El assert se reduce a algo as√≠:

```php
// Assert
Approvals::verifyAsJson($output);
```

La primera vez que ejecutas el test no hay snapshot. El test falla y te muestra el resultado completo que ha producido el sistema. Ese es el momento importante: revisas ese output con conocimiento del dominio y decides **‚Äúesto es lo que hoy consideramos correcto‚Äù**. Cuando lo apruebas, el snapshot se guarda.

A partir de ese momento, cualquier cambio futuro en el comportamiento del flujo genera un diff claro y legible. No un stack trace cr√≠ptico, sino una comparaci√≥n directa entre *lo que hab√≠a antes* y *lo que hay ahora*. Y ese diff te obliga a tomar una decisi√≥n expl√≠cita: o has encontrado un bug, o est√°s cambiando el comportamiento de forma intencionada y debes aprobar el nuevo resultado.

Este enfoque encaja especialmente bien con **tests de caracterizaci√≥n**: no te obliga a decidir desde el principio qu√© est√° bien o mal, pero s√≠ **te obliga a ser consciente de cada cambio de comportamiento que introduces**.

---

## Normalizaci√≥n: la clave para que no sea un infierno

Los snapshots que generan estos tests contienen mucha informaci√≥n que **no aporta significado real** al comportamiento del sistema: identificadores generados al vuelo, timestamps, hashes, tokens o valores dependientes del entorno. Si todo eso se compara tal cual, cada ejecuci√≥n del test producir√≠a diferencias irrelevantes y los diffs ser√≠an in√∫tiles.

Por eso, antes de comparar snapshots, el output pasa siempre por una fase de normalizaci√≥n. El objetivo es eliminar o estabilizar cualquier dato que no represente una decisi√≥n real del sistema.

Sin normalizaci√≥n, este enfoque ser√≠a simplemente inviable.

En la pr√°ctica, eso implica cosas como:

* eliminar IDs autogenerados,
* reemplazar timestamps por valores fijos o placeholders,
* anonimizar hashes o tokens,
* y normalizar cualquier campo cuyo valor dependa del entorno o del momento de ejecuci√≥n.

Conceptualmente, el c√≥digo hace algo as√≠:

```php
$this->normalizer->stripIds($output);
$this->normalizer->normalizeTimestamps($output);
```

No es una limpieza cosm√©tica. Es una decisi√≥n de dise√±o del test: **queremos que el snapshot capture intenci√≥n y comportamiento**, no detalles accidentales de la ejecuci√≥n.

Cuando esta normalizaci√≥n est√° bien hecha, ocurre algo interesante: los diffs dejan de ser ruido y pasan a contar una historia clara. Cada cambio visible suele corresponder a una decisi√≥n distinta en el flujo, y eso hace que el test sea realmente √∫til en el d√≠a a d√≠a.

---

## Qu√© tipo de cambios detecta este test

Este tipo de test E2E detecta cosas que otros tests no ven:

* una regla que deja de evaluarse,
* un cambio en el orden de decisiones,
* una integraci√≥n que ya no se llama,
* un estado que deja de persistirse,
* un flujo que se corta antes de tiempo.

Y lo hace **sin conocer la implementaci√≥n interna**.

Eso es justo lo que necesitas cuando refactorizas legacy con miedo.

---

## ¬øCu√°nto tardan estos tests?

Una pregunta inevitable con este tipo de tests es el tiempo de ejecuci√≥n. La intuici√≥n suele ser que los tests E2E son lentos y costosos, y muchas veces lo son. En este caso concreto, no.

El test completo ‚Äîejecutando el flujo, tocando base de datos y capturando estado, logs y eventos‚Äî tarda **en torno a medio segundo**. Es un coste perfectamente asumible para un test que cubre un flujo cr√≠tico de principio a fin.

Adem√°s, estos tests se ejecutan **de forma autom√°tica en CI**. No son algo que alguien lanza ‚Äúde vez en cuando‚Äù en local, sino una red de seguridad real que se ejecuta en cada cambio relevante. El objetivo no es tener cientos de estos tests, sino unos pocos bien escogidos que aporten m√°xima se√±al con un coste razonable.

---

## ¬øCu√°ntos tests de estos tiene sentido tener?

No muchos. Y eso es una decisi√≥n consciente.

Este tipo de tests son muy potentes: cubren flujos completos, atraviesan muchas capas del sistema y detectan cambios de comportamiento que otros tests no ven. Pero precisamente por eso tambi√©n tienen **un coste de mantenimiento**. Cada vez que el comportamiento leg√≠timamente cambia, hay que revisar y aprobar nuevos snapshots.

La idea no es cubrir todos los flujos del sistema, ni sustituir otros tipos de tests. El objetivo es proteger **aquellos flujos que no te puedes permitir romper**, los que tienen impacto directo en negocio y donde un bug se paga caro.

En un e-commerce t√≠pico, esos flujos suelen ser bastante claros:

* creaci√≥n de pedidos,
* pagos,
* fulfillment,
* KYC / antifraude.

Pocos tests, bien escogidos y bien entendidos. Como red de seguridad de alto nivel, no como malla fina. Cuando se usan as√≠, el coste compensa de sobra el valor que aportan.

---

## El beneficio real (m√°s all√° de los tests)

Despu√©s de montar este primer test E2E, el cambio no fue √∫nicamente t√©cnico. Lo que cambi√≥ fue la relaci√≥n con el sistema.

Cambi√≥ la forma de refactorizar, porque ya no se trabajaba a ciegas. Cambi√≥ la velocidad de ciertos cambios, porque el miedo dej√≥ de ser el factor dominante. Cambi√≥ la confianza al tocar c√≥digo sensible, porque hab√≠a una se√±al clara que avisar√≠a si algo importante se romp√≠a. Y, de forma m√°s sutil pero igual de relevante, cambi√≥ la conversaci√≥n con negocio.

Antes, cualquier cambio en un flujo cr√≠tico se justificaba con un acto de fe:

> *‚ÄúCreo que esto no rompe nada‚Äù*.

Despu√©s, la conversaci√≥n pas√≥ a otro nivel:

> *‚ÄúSi rompe algo importante, el test nos lo va a decir‚Äù*.

Esa diferencia parece peque√±a, pero en sistemas legacy cr√≠ticos es enorme. No elimina el riesgo por completo, pero lo hace visible, gestionable y compartido. Y eso, muchas veces, es justo lo que permite empezar a mejorar un sistema que llevaba demasiado tiempo intocable.

---

## Mejorar un legacy sin jugar a la ruleta rusa

Este enfoque no es el m√°s ‚Äúlimpio‚Äù, ni el m√°s acad√©mico. No sigue al pie de la letra ning√∫n libro ni persigue una arquitectura ideal desde el primer d√≠a.

Pero es **incremental, seguro** y, sobre todo, **realista**.

No intenta arreglar todo de golpe, ni promete convertir un sistema legacy en algo ejemplar en pocas semanas. Parte de la realidad tal y como es y trabaja desde ah√≠, reduciendo riesgo paso a paso.

Hace algo mucho m√°s valioso:

üëâ **te permite mejorar un sistema legacy sin jugar a la ruleta rusa en producci√≥n**.

Si est√°s en un proyecto donde sabes que necesitas tests pero no ves por d√≥nde empezar, este tipo de test E2E de caracterizaci√≥n suele ser un punto de entrada muy razonable. No es el final del camino, ni pretende serlo.

Pero s√≠ es, con diferencia, un muy buen principio.
